{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Desired image dimensions\n",
    "IMAGE_SIZE = (299, 299)\n",
    "# Max vocabulary size\n",
    "MAX_VOCAB_SIZE = 2000000\n",
    "# Fixed length allowed for any sequence\n",
    "SEQ_LENGTH = 25\n",
    "# Dimension for the image embeddings and token embeddings\n",
    "EMBED_DIM = 128\n",
    "# Number of self-attention heads\n",
    "NUM_HEADS = 8\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 128 #1024 \n",
    "# Shuffle dataset dim on tf.data.Dataset\n",
    "SHUFFLE_DIM = 512\n",
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "# Numbers of training epochs\n",
    "EPOCHS = 14\n",
    "\n",
    "# Reduce Dataset\n",
    "# If you want reduce number of train/valid images dataset, set 'REDUCE_DATASET=True'\n",
    "# and set number of train/valid images that you want.\n",
    "#### COCO dataset\n",
    "# Max number train dataset images : 68363\n",
    "# Max number valid dataset images : 33432\n",
    "REDUCE_DATASET = False\n",
    "# Number of train images -> it must be a value between [1, 68363]\n",
    "NUM_TRAIN_IMG = 68363\n",
    "# Number of valid images -> it must be a value between [1, 33432]\n",
    "# N.B. -> IMPORTANT : the number of images of the test set is given by the difference between 33432 and NUM_VALID_IMG values.\n",
    "# for instance, with NUM_VALID_IMG = 20000 -> valid set have 20000 images and test set have the last 13432 images.\n",
    "NUM_VALID_IMG = 20000\n",
    "# Data augmentation on train set\n",
    "TRAIN_SET_AUG = True\n",
    "# Data augmentation on valid set\n",
    "VALID_SET_AUG = False\n",
    "# If you want to calculate the performance on the test set.\n",
    "TEST_SET = False\n",
    "\n",
    "# Load train_data.json pathfile\n",
    "train_data_json_path = \"COCO_dataset/captions_mapping_train.json\"\n",
    "# Load valid_data.json pathfile\n",
    "valid_data_json_path = \"COCO_dataset/captions_mapping_valid.json\"\n",
    "# Load text_data.json pathfile\n",
    "text_data_json_path  = \"COCO_dataset/text_data.json\"\n",
    "\n",
    "# Save training files directory\n",
    "SAVE_DIR = \"save_train_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 25, 128])\n",
      "torch.Size([64, 25, 128])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.dense_proj = nn.Linear(embed_dim, dense_dim)\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "        # print(self.layernorm_1)\n",
    "        # self.inp_layer=nn.Linear(128,128)\n",
    "    def forward(self, inputs, mask=None):\n",
    "        # inputs=self.inp_layer(inputs)\n",
    "        # print(inputs)\n",
    "        inputs = self.dense_proj(inputs)\n",
    "        attention_output, _ = self.attention(\n",
    "            inputs, inputs, inputs, attn_mask=None\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        return proj_input\n",
    "\n",
    "# Example Usage\n",
    "block = TransformerEncoderBlock(EMBED_DIM, FF_DIM, NUM_HEADS)\n",
    "# Assuming input tensor x of shape (batch_size, seq_length, embed_dim)\n",
    "x = torch.randn(BATCH_SIZE, SEQ_LENGTH, EMBED_DIM)\n",
    "print(x.shape)\n",
    "output = block(x)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            num_embeddings=sequence_length, embedding_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_length = inputs.size()\n",
    "        positions = torch.arange(seq_length, device=inputs.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return inputs != 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedded output: torch.Size([32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example usage to test PositionalEmbedding class\n",
    "\n",
    "# Assuming the input parameters\n",
    "sequence_length = 128\n",
    "vocab_size = 1000\n",
    "embed_dim = 128\n",
    "\n",
    "# Create an instance of the PositionalEmbedding class\n",
    "pos_embedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim)\n",
    "\n",
    "# Create a sample input tensor\n",
    "import torch\n",
    "inputs = torch.randint(0, vocab_size, (32, sequence_length))  # Assuming batch size of 32\n",
    "\n",
    "# Compute positional embeddings\n",
    "embedded_output = pos_embedding(inputs)\n",
    "\n",
    "# Check the shape of the output\n",
    "print(\"Shape of embedded output:\", embedded_output.shape)\n",
    "# print(embedded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Multi-Head Attention layers\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.attention_2 = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.dense_proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_3 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.embedding = PositionalEmbedding(SEQ_LENGTH, vocab_size, embed_dim)  # Adjusted\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout_1 = nn.Dropout(0.1)\n",
    "        self.dropout_2 = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        inputs = self.dropout_1(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = mask.unsqueeze(1)\n",
    "            combined_mask = torch.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            combined_mask = None\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output_1, _ = self.attention_1(inputs, inputs, inputs, attn_mask=combined_mask)\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs, attn_mask=padding_mask)\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        proj_out = self.layernorm_3(out_2 + proj_output)\n",
    "        proj_out = self.dropout_2(proj_out)\n",
    "\n",
    "        preds = self.out(proj_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        batch_size, sequence_length, _ = inputs.size()\n",
    "        mask = torch.tril(torch.ones(sequence_length, sequence_length, device=inputs.device))\n",
    "        mask = mask.unsqueeze(0).expand(batch_size, -1, -1)  # [batch_size, seq_length, seq_length]\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs tensor: torch.Size([2, 25])\n",
      "Shape of encoder outputs tensor: torch.Size([2, 25, 6])\n",
      "Shape of predictions: torch.Size([2, 25, 23])\n"
     ]
    }
   ],
   "source": [
    "# Example usage to test TransformerDecoderBlock class\n",
    "\n",
    "# Assuming the input parameters\n",
    "embed_dim = 6\n",
    "ff_dim = 6\n",
    "num_heads = 3\n",
    "vocab_size = 23\n",
    "\n",
    "# Create an instance of the TransformerDecoderBlock class\n",
    "decoder_block = TransformerDecoderBlock(embed_dim, ff_dim, num_heads, vocab_size)\n",
    "\n",
    "# Create sample inputs (batch_size, sequence_length)\n",
    "import torch\n",
    "inputs = torch.randint(0, vocab_size, (2, 25))  # Assuming batch size of 32 and sequence length of 25\n",
    "encoder_outputs = torch.randn(2, 25, embed_dim)  # Example encoder outputs (batch_size, sequence_length, embed_dim)\n",
    "\n",
    "# Ensure inputs tensor shape is (batch_size, sequence_length)\n",
    "print(\"Shape of inputs tensor:\", inputs.shape)\n",
    "\n",
    "# Ensure encoder outputs tensor shape is (batch_size, sequence_length, embed_dim)\n",
    "print(\"Shape of encoder outputs tensor:\", encoder_outputs.shape)\n",
    "\n",
    "# Compute predictions\n",
    "predictions = decoder_block(inputs, encoder_outputs, training=True)\n",
    "\n",
    "# Check the shape of the predictions\n",
    "print(\"Shape of predictions:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-2.0606e-01, -1.7114e-01, -9.4247e-01,  ..., -6.6829e-01,\n",
      "           5.7754e-01,  4.7863e-04],\n",
      "         [-1.3249e+00, -6.6958e-01, -8.1786e-01,  ..., -7.1838e-01,\n",
      "           8.7098e-01,  1.6100e+00],\n",
      "         [ 3.4973e-01,  3.9239e-02, -2.0889e-01,  ..., -3.5631e-01,\n",
      "           1.1971e-01, -4.3944e-01],\n",
      "         ...,\n",
      "         [-5.7583e-01, -1.1395e+00, -1.6274e+00,  ..., -1.2789e+00,\n",
      "           1.0947e+00,  1.1039e+00],\n",
      "         [-3.0398e-01,  3.2044e-01, -1.7796e+00,  ..., -1.2521e+00,\n",
      "          -5.1979e-01, -7.2534e-01],\n",
      "         [ 1.5497e+00, -5.7710e-01, -3.9896e-01,  ..., -1.5744e-01,\n",
      "           1.2541e+00, -1.0405e+00]],\n",
      "\n",
      "        [[-6.6756e-01,  4.2434e-01, -8.1626e-01,  ..., -2.3648e+00,\n",
      "           8.8223e-02,  8.7488e-01],\n",
      "         [-9.6958e-01, -9.0369e-01, -1.5932e+00,  ..., -1.0449e+00,\n",
      "           8.2456e-01,  1.1915e+00],\n",
      "         [ 5.8468e-01, -3.5219e-01, -8.7830e-01,  ..., -4.6111e-01,\n",
      "           7.3699e-01, -5.6171e-01],\n",
      "         ...,\n",
      "         [-2.3316e-01, -6.9331e-01, -3.5512e-01,  ...,  2.5276e-01,\n",
      "          -2.2175e-01, -4.7448e-02],\n",
      "         [ 9.0914e-02, -4.8839e-01,  4.7549e-01,  ...,  4.9097e-01,\n",
      "          -1.7636e-01,  4.1687e-02],\n",
      "         [ 1.5919e+00, -1.9777e+00,  1.1590e+00,  ...,  1.9169e+00,\n",
      "           4.0875e-01, -6.6948e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from swin_transformer import *\n",
    "\n",
    "model_swin=SwinModel(embed_dim, num_patch_x, num_patch_y, num_heads, num_mlp, window_size, shift_size, qkv_bias, num_classes)\n",
    "model_swin.load_state_dict(torch.load(\"/home/ayushh/MedMNIST_SWIN_Transformer/swin_classification_pytorch_model_weights.pth\"))\n",
    "device=torch.device('cuda')\n",
    "model_swin.to(device)\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,model_swin, encoder, decoder, num_captions_per_image=5,    # cnn_model,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.swin_model = model_swin #cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        img_embed = self.swin_model(inputs[0])\n",
    "        encoder_out = self.encoder(img_embed, False)\n",
    "        decoder_out = self.decoder(inputs[2], encoder_out, training=inputs[1], mask=None)\n",
    "        return decoder_out\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = F.cross_entropy(y_pred.transpose(1, 2), y_true, ignore_index=0, reduction='none')\n",
    "        loss *= mask\n",
    "        return loss.sum() / mask.sum()\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        pred_ids = torch.argmax(y_pred, dim=-1)\n",
    "        correct = (pred_ids == y_true) & mask\n",
    "        return correct.sum().float() / mask.sum()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        img_embed = self.swin_model(batch_img)\n",
    "\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            encoder_out = self.encoder(img_embed, training=True)\n",
    "\n",
    "            batch_seq_inp = batch_seq[:, i, :-1]\n",
    "            batch_seq_true = batch_seq[:, i, 1:]\n",
    "\n",
    "            mask = batch_seq_true != 0\n",
    "\n",
    "            batch_seq_pred = self.decoder(\n",
    "                batch_seq_inp, encoder_out, training=True, mask=mask\n",
    "            )\n",
    "\n",
    "            caption_loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "            caption_acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "\n",
    "            batch_loss += caption_loss\n",
    "            batch_acc += caption_acc\n",
    "\n",
    "        loss = batch_loss\n",
    "        acc = batch_acc / float(self.num_captions_per_image)\n",
    "\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        img_embed = self.swin_model(batch_img)\n",
    "\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            encoder_out = self.encoder(img_embed, training=False)\n",
    "\n",
    "            batch_seq_inp = batch_seq[:, i, :-1]\n",
    "            batch_seq_true = batch_seq[:, i, 1:]\n",
    "\n",
    "            mask = batch_seq_true != 0\n",
    "\n",
    "            batch_seq_pred = self.decoder(\n",
    "                batch_seq_inp, encoder_out, training=False, mask=mask\n",
    "            )\n",
    "\n",
    "            caption_loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "            caption_acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "\n",
    "            batch_loss += caption_loss\n",
    "            batch_acc += caption_acc\n",
    "\n",
    "        loss = batch_loss\n",
    "        acc = batch_acc / float(self.num_captions_per_image)\n",
    "\n",
    "        return {\"loss\": loss, \"acc\": acc}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return []  # Returning an empty list as PyTorch handles metrics differently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
